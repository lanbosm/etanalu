{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lanbosm/etanalu/blob/master/lanbosm_forge_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![visitor][visitor-badge]][visitor-stats]\n",
        "\n",
        "# **Cagliostro Forge Colab**\n",
        "Rise from the ashes, reborn and empowered by [lllyasviel/stable-diffusion-webui-forge](https://github.com/lllyasviel/stable-diffusion-webui-forge)\n",
        "\n",
        "**Version 1.0.0** | [Github][link-to-github] | [License](https://github.com/cagliostrolab/forge-colab/blob/main/LICENSE)\n",
        "\n",
        "[visitor-badge]: https://api.visitorbadge.io/api/visitors?path=cagliostro-forge-colab&label=Visitors&labelColor=%2334495E&countColor=%231ABC9C&style=flat&labelStyle=none\n",
        "[visitor-stats]: https://visitorbadge.io/status?path=cagliostro-forge-colab\n",
        "[link-to-github]: https://github.com/cagliostrolab/forge-colab/blob/main/cagliostro-forge-colab.ipynb"
      ],
      "metadata": {
        "id": "GnmYPpV3o719"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !ln -s /usr/local/bin/huggingface-cli /usr/bin/huggingface-cli\n"
      ],
      "metadata": {
        "id": "dyTGHwAW3w1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## **Install Environment**\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import string\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel\n",
        "\n",
        "python_version  = \".\".join(sys.version.split(\".\")[:2])\n",
        "python_path     = Path(f\"/usr/local/lib/python{python_version}/dist-packages/\")\n",
        "colablib_path   = python_path / \"colablib\"\n",
        "if not colablib_path.exists():\n",
        "    subprocess.run(['pip', 'install', '--upgrade', 'git+https://github.com/Linaqruf/colablib'], check=True)\n",
        "\n",
        "from colablib.colored_print import cprint, print_line\n",
        "from colablib.utils import py_utils, package_utils, config_utils\n",
        "from colablib.sd_models.downloader import aria2_download, download\n",
        "from colablib.utils.git_utils import update_repo, reset_repo, validate_repo, batch_update\n",
        "from colablib.utils.py_utils import get_filename\n",
        "\n",
        "\n",
        "################################\n",
        "# COLAB ARGUMENTS GOES HERE\n",
        "################################\n",
        "\n",
        "# It ain't much, but it's honest work.\n",
        "class CustomDirs(BaseModel):\n",
        "    url: str\n",
        "    dst: str\n",
        "\n",
        "# @markdown ### **Drive Config**\n",
        "mount_drive          = True  # @param {type: 'boolean'}\n",
        "output_drive_folder  = \"cagliostro-colab-forge\"  # @param {type: 'string'}\n",
        "\n",
        "# @markdown ### **Repo Config**\n",
        "update_webui         = True  # @param {type: 'boolean'}\n",
        "update_extensions    = False  # @param {type: 'boolean'}\n",
        "commit_hash          = \"\"  # @param {type: 'string'}\n",
        "\n",
        "# @markdown ### **Download Config**\n",
        "# @markdown > Check only the options you need\n",
        "animagine_xl_3_1     = False  # @param {type: 'boolean'}\n",
        "rae_diffusion_xl_v2  = False  # @param {type: 'boolean'}\n",
        "kivotos_xl_v2_0      = False  # @param {type: 'boolean'}\n",
        "urangdiffusion_2_0   = False  # @param {type: 'boolean'}\n",
        "\n",
        "# @markdown > **Note:**\n",
        "# @markdown - For multiple URLs, use comma separation (e.g. `url1, url2, url3`)\n",
        "# @markdown - Forge supports FLUX, SD, and SDXL, but this notebook focuses only on SDXL\n",
        "# @markdown - **Highly Recommended:** Use Hugging Face links whenever possible\n",
        "custom_model_url     = \"\"  # @param {'type': 'string'}\n",
        "custom_vae_url       = \"https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/resolve/main/sdxl.vae.safetensors\"  # @param {'type': 'string'}\n",
        "custom_lora_url      = \"\"  # @param {'type': 'string'}\n",
        "\n",
        "# @markdown ### **Tunnel Config**\n",
        "# @markdown > Default to `--share` until `ngrok_token` is not `None`\n",
        "ngrok_token          = \"\"  # @param {type: 'string'}\n",
        "ngrok_region         = \"ap\"  # @param [\"us\", \"eu\", \"au\", \"ap\", \"sa\", \"jp\", \"in\"]\n",
        "\n",
        "# @markdown ### **UI/UX Config**\n",
        "gradio_theme         = \"Default\"  # @param [\"Default\", \"gradio/base\", \"gradio/glass\", \"gradio/monochrome\", \"gradio/seafoam\", \"gradio/soft\", \"gradio/dracula_test\", \"abidlabs/dracula_test\", \"abidlabs/Lime\", \"abidlabs/pakistan\", \"Ama434/neutral-barlow\", \"dawood/microsoft_windows\", \"finlaymacklon/smooth_slate\", \"Franklisi/darkmode\", \"freddyaboulton/dracula_revamped\", \"freddyaboulton/test-blue\", \"gstaff/xkcd\", \"Insuz/Mocha\", \"Insuz/SimpleIndigo\", \"JohnSmith9982/small_and_pretty\", \"nota-ai/theme\", \"nuttea/Softblue\", \"ParityError/Anime\", \"reilnuud/polite\", \"remilia/Ghostly\", \"rottenlittlecreature/Moon_Goblin\", \"step-3-profit/Midnight-Deep\", \"Taithrah/Minimal\", \"ysharma/huggingface\", \"ysharma/steampunk\", \"NoCrypt/miku\"]\n",
        "# @markdown Set `use_preset` for using default prompt, resolution, sampler, and other settings\n",
        "use_presets          = True  # @param {type: 'boolean'}\n",
        "\n",
        "# @markdown ### **Launch Arguments**\n",
        "use_gradio_auth      = False  # @param {type: 'boolean'}\n",
        "auto_select_model    = False  # @param {type: 'boolean'}\n",
        "auto_select_vae      = True  # @param {type: 'boolean'}\n",
        "additional_arguments = \"--lowram --theme dark --no-half-vae --opt-sdp-attention\"  # @param {type: 'string'}\n",
        "\n",
        "################################\n",
        "# GLOBAL VARIABLES GOES HERE\n",
        "################################\n",
        "\n",
        "# GRADIO AUTH\n",
        "user      = \"cagliostro\"\n",
        "password  = \"\".join(random.choices(string.ascii_letters + string.digits, k=6))\n",
        "\n",
        "# ROOT DIR\n",
        "root_dir        = Path(\"/content\")\n",
        "drive_dir       = root_dir / \"drive\" / \"MyDrive\"\n",
        "repo_dir        = root_dir / \"stable-diffusion-webui-forge\"\n",
        "tmp_dir         = root_dir / \"tmp\"\n",
        "\n",
        "models_dir      = repo_dir / \"models\"\n",
        "extensions_dir  = repo_dir / \"extensions\"\n",
        "ckpt_dir        = models_dir / \"Stable-diffusion\"\n",
        "vae_dir         = models_dir / \"VAE\"\n",
        "lora_dir        = models_dir / \"Lora\"\n",
        "output_subdir   = [\"txt2img-samples\", \"img2img-samples\", \"extras-samples\", \"txt2img-grids\", \"img2img-grids\"]\n",
        "\n",
        "config_file_path    = repo_dir / \"config.json\"\n",
        "ui_config_file_path = repo_dir / \"ui-config.json\"\n",
        "\n",
        "package_url = [\n",
        "    \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/webui-forge.tar.lz4\",\n",
        "    \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/webui-forge-deps.tar.lz4\",\n",
        "]\n",
        "\n",
        "custom_dirs = {\n",
        "    \"model\" : CustomDirs(url=custom_model_url, dst=str(ckpt_dir)),\n",
        "    \"vae\"   : CustomDirs(url=custom_vae_url, dst=str(vae_dir)),\n",
        "    \"lora\"  : CustomDirs(url=custom_lora_url, dst=str(lora_dir)),\n",
        "}\n",
        "\n",
        "default_model_urls = {\n",
        "    \"animagine_xl_3_1\"      : \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/resolve/main/animagine-xl-3.1.safetensors\",\n",
        "    \"rae_diffusion_xl_v2\"   : \"https://huggingface.co/Raelina/Rae-Diffusion-XL-V2/resolve/main/RaeDiffusion-XL-v2.safetensors\",\n",
        "    \"kivotos_xl_v2_0\"       : \"https://huggingface.co/yodayo-ai/kivotos-xl-2.0/resolve/main/kivotos-xl-2.0.safetensors\",\n",
        "    \"urangdiffusion_2_0\"    : \"https://huggingface.co/kayfahaarukku/UrangDiffusion-2.0/resolve/main/UrangDiffusion-2.0.safetensors\",\n",
        "}\n",
        "\n",
        "################################\n",
        "# HELPER FUNCTIONS STARTS HERE\n",
        "################################\n",
        "\n",
        "def mount_drive_function(directory):\n",
        "    output_dir = repo_dir / \"outputs\"\n",
        "\n",
        "    if mount_drive:\n",
        "        print_line(80, color=\"green\")\n",
        "        if not directory.exists():\n",
        "            from google.colab import drive\n",
        "            cprint(\"Mounting google drive...\", color=\"green\", reset=False)\n",
        "            drive.mount(str(directory.parent))\n",
        "        output_dir = directory / output_drive_folder\n",
        "        cprint(\"Set default output path to:\", output_dir, color=\"green\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def setup_directories():\n",
        "    for dir in [ckpt_dir, vae_dir, lora_dir]:\n",
        "        dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def pre_download(dir, urls, desc, overwrite=False):\n",
        "    ffmpy_path = python_path / \"ffmpy-0.3.0.dist-info\"\n",
        "\n",
        "    for url in tqdm(urls, desc=desc):\n",
        "        filename = Path(url).name\n",
        "        aria2_download(dir, filename, url, quiet=True)\n",
        "        if filename == \"webui-forge-deps.tar.lz4\":\n",
        "            package_utils.extract_package(filename, python_path, overwrite=True)\n",
        "        else:\n",
        "            package_utils.extract_package(filename, \"/\", overwrite=overwrite)\n",
        "        os.remove(dir / filename)\n",
        "\n",
        "    subprocess.run([\"rm\", \"-rf\", str(ffmpy_path)])\n",
        "    subprocess.run([\"pip\", \"install\", \"--force-reinstall\", \"ffmpy\"], check=True)\n",
        "\n",
        "def install_dependencies():\n",
        "    ubuntu_deps = [\"aria2\", \"lz4\"]\n",
        "    cprint(\"Installing ubuntu dependencies\", color=\"green\")\n",
        "    subprocess.run([\"apt\", \"install\", \"-y\"] + ubuntu_deps, check=True)\n",
        "\n",
        "def install_webui(repo_dir, desc):\n",
        "    if not repo_dir.exists():\n",
        "        pre_download(root_dir, package_url, desc, overwrite=False)\n",
        "    else:\n",
        "        cprint(\"Stable Diffusion Web UI Forge already installed, skipping...\", color=\"green\")\n",
        "\n",
        "def configure_output_path(config_path, output_dir, output_subdir):\n",
        "    try:\n",
        "        config = config_utils.read_config(str(config_path))\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        config = {}\n",
        "\n",
        "    config_updates = {\n",
        "        f\"outdir_{subdir.split('-')[0]}_{'_'.join(subdir.split('-')[1:])}\": str(output_dir / subdir)\n",
        "        for subdir in output_subdir\n",
        "    }\n",
        "    config.update(config_updates)\n",
        "\n",
        "    config_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config_utils.write_config(str(config_path), config)\n",
        "\n",
        "    for dir in output_subdir:\n",
        "        (output_dir / dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def prepare_environment():\n",
        "    cprint(\"Preparing environment...\", color=\"green\")\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF']   = \"garbage_collection_threshold:0.9,max_split_size_mb:512\"\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]      = \"3\"\n",
        "    os.environ[\"PYTHONWARNINGS\"]            = \"ignore\"\n",
        "\n",
        "def custom_download(custom_dirs):\n",
        "    filtered_urls = filter_dict_items(default_model_urls)\n",
        "\n",
        "    for key, value in custom_dirs.items():\n",
        "        urls = value.url.split(\",\")\n",
        "        dst = value.dst\n",
        "\n",
        "        if key == \"model\":\n",
        "            urls.extend(filtered_urls)\n",
        "\n",
        "        if urls[0]:\n",
        "            print_line(80, color=\"green\")\n",
        "            cprint(f\" [-] Downloading Custom {key}...\", color=\"flat_yellow\")\n",
        "\n",
        "        for url in urls:\n",
        "            url = url.strip()\n",
        "            if url != \"\":\n",
        "                print_line(80, color=\"green\")\n",
        "                if \"|\" in url:\n",
        "                    url, filename = map(str.strip, url.split(\"|\"))\n",
        "                    if not filename.endswith((\".safetensors\", \".ckpt\", \".pt\", \"pth\")):\n",
        "                        filename = filename + Path(get_filename(url)).suffix\n",
        "                else:\n",
        "                    filename = get_filename(url)\n",
        "\n",
        "                download(url=url, filename=filename, dst=dst, quiet=False)\n",
        "\n",
        "def filter_dict_items(dict_items):\n",
        "    result_list = []\n",
        "    for key, url in dict_items.items():\n",
        "        if globals().get(key):\n",
        "            result_list.append(url)\n",
        "    return result_list\n",
        "\n",
        "def auto_select_file(target_dir, config_key, file_types):\n",
        "    valid_files = [f for f in os.listdir(target_dir) if f.endswith(file_types)]\n",
        "    if valid_files:\n",
        "        file_path = random.choice(valid_files)\n",
        "\n",
        "        if Path(target_dir).joinpath(file_path).exists():\n",
        "            config = config_utils.read_config(str(config_file_path))\n",
        "            config[config_key] = file_path\n",
        "            config_utils.write_config(str(config_file_path), config)\n",
        "        return file_path\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def ui_config_presets():\n",
        "    preset_prompt = \"masterpiece, best quality, very aesthetic, absurdres\"\n",
        "    preset_negative_prompt = \"nsfw, lowres, (bad), text, error, fewer, extra, missing, worst quality, jpeg artifacts, low quality, watermark, unfinished, displeasing, oldest, early, chromatic aberration, signature, extra digits, artistic error, username, scan, [abstract]\"\n",
        "\n",
        "    return {\n",
        "        \"txt2img/Prompt/value\"              : preset_prompt,\n",
        "        \"txt2img/Negative prompt/value\"     : preset_negative_prompt,\n",
        "        \"img2img/Prompt/value\"              : preset_prompt,\n",
        "        \"img2img/Negative prompt/value\"     : preset_negative_prompt,\n",
        "        \"customscript/sampler.py/txt2img/Sampling method/value\" : \"Euler a\",\n",
        "        \"customscript/sampler.py/txt2img/Sampling steps/value\"  : 28,\n",
        "        \"customscript/sampler.py/txt2img/Scheduler/value\"       : \"Automatic\",\n",
        "    }\n",
        "\n",
        "def ui_config_settings(ui_config_file: str):\n",
        "    config = config_utils.read_config(str(ui_config_file))\n",
        "    preset_config = ui_config_presets()\n",
        "\n",
        "    for key, value in preset_config.items():\n",
        "        config[key] = value\n",
        "\n",
        "    config_utils.write_config(str(ui_config_file), config)\n",
        "\n",
        "def general_config_presets(config_file: str, lora_dir: str, use_presets: bool, ui_config_file: str):\n",
        "    config = config_utils.read_config(str(config_file))\n",
        "\n",
        "    config.update({\n",
        "        \"CLIP_stop_at_last_layers\"      : 2,\n",
        "        \"show_progress_every_n_steps\"   : 10,\n",
        "        \"show_progressbar\"              : True,\n",
        "        \"samples_filename_pattern\"      : \"[model_name]_[seed]\",\n",
        "        \"show_progress_type\"            : \"Approx NN\",\n",
        "        \"live_preview_content\"          : \"Prompt\",\n",
        "        \"forge_preset\"                  : \"xl\",\n",
        "        \"xl_t2i_width\"                  : 832,\n",
        "        \"xl_t2i_height\"                 : 1216,\n",
        "        \"xl_t2i_cfg\"                    : 7,\n",
        "        \"xl_t2i_hr_cfg\"                 : 7,\n",
        "        \"xl_t2i_sampler\"                : \"Euler a\",\n",
        "        \"xl_t2i_scheduler\"              : \"Automatic\",\n",
        "        \"gradio_theme\"                  : gradio_theme,\n",
        "    })\n",
        "\n",
        "    config_utils.write_config(str(config_file), config)\n",
        "\n",
        "    if use_presets:\n",
        "        ui_config_settings(ui_config_file)\n",
        "\n",
        "def is_valid(target_dir, file_types):\n",
        "    return any(f.endswith(file_types) for f in os.listdir(target_dir))\n",
        "\n",
        "def parse_args(config):\n",
        "    args = []\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args.append(f'\"{v}\"')\n",
        "        elif isinstance(v, str):\n",
        "            args.append(f'--{k}=\"{v}\"')\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args.append(f\"--{k}\")\n",
        "        elif isinstance(v, (float, int)) and not isinstance(v, bool):\n",
        "            args.append(f\"--{k}={v}\")\n",
        "    return \" \".join(args)\n",
        "\n",
        "def main():\n",
        "    global output_dir, auto_select_model, auto_select_vae\n",
        "\n",
        "    ################################\n",
        "    # MAIN EXECUTION\n",
        "    ################################\n",
        "\n",
        "    os.chdir(root_dir)\n",
        "    start_time = time.time()\n",
        "    output_dir = mount_drive_function(drive_dir)\n",
        "\n",
        "    gpu_info    = py_utils.get_gpu_info(get_gpu_name=True)\n",
        "    python_info = py_utils.get_python_version()\n",
        "    torch_info  = py_utils.get_torch_version()\n",
        "\n",
        "    print_line(80, color=\"green\")\n",
        "    cprint(f\" [-] Current GPU: {gpu_info}\", color=\"flat_yellow\")\n",
        "    cprint(f\" [-] Python {python_info}\", color=\"flat_yellow\")\n",
        "    cprint(f\" [-] Torch {torch_info}\", color=\"flat_yellow\")\n",
        "    print_line(80, color=\"green\")\n",
        "\n",
        "    try:\n",
        "        install_dependencies()\n",
        "\n",
        "        print_line(80, color=\"green\")\n",
        "        install_webui(repo_dir, cprint(\"Unpacking Web UI Forge\", color=\"green\", tqdm_desc=True))\n",
        "        prepare_environment()\n",
        "\n",
        "        configure_output_path(config_file_path, output_dir, output_subdir)\n",
        "\n",
        "        print_line(80, color=\"green\")\n",
        "        if update_webui and not commit_hash:\n",
        "            update_repo(cwd=repo_dir, args=\"-X theirs --rebase --autostash\")\n",
        "        elif commit_hash:\n",
        "            reset_repo(repo_dir, commit_hash)\n",
        "\n",
        "        setup_directories()\n",
        "\n",
        "        repo_name, current_commit_hash, current_branch = validate_repo(repo_dir)\n",
        "        cprint(f\"Using '{repo_name}' repository...\", color=\"green\")\n",
        "        cprint(f\"Branch: {current_branch}, Commit hash: {current_commit_hash}\", color=\"green\")\n",
        "\n",
        "        if update_extensions:\n",
        "            print_line(80, color=\"green\")\n",
        "            batch_update(fetch=True, directory=extensions_dir, desc=cprint(\"Updating extensions\", color=\"green\", tqdm_desc=True))\n",
        "\n",
        "        elapsed_time = py_utils.calculate_elapsed_time(start_time)\n",
        "        print_line(80, color=\"green\")\n",
        "        cprint(f\"Finished installation. Took {elapsed_time}.\", color=\"flat_yellow\")\n",
        "    except Exception as e:\n",
        "        cprint(f\"An error occurred: {str(e)}\", color=\"red\")\n",
        "        print_line(80, color=\"red\")\n",
        "        cprint(\"Setup failed. Please check the error message above and try again.\", color=\"red\")\n",
        "        print_line(80, color=\"red\")\n",
        "        return\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    custom_download(custom_dirs)\n",
        "\n",
        "    elapsed_time = py_utils.calculate_elapsed_time(start_time)\n",
        "    print_line(80, color=\"green\")\n",
        "    cprint(f\"Download finished. Took {elapsed_time}.\", color=\"flat_yellow\")\n",
        "    print_line(80, color=\"green\")\n",
        "    cprint(f\"Launching '{repo_name}'\", color=\"flat_yellow\")\n",
        "    print_line(80, color=\"green\")\n",
        "\n",
        "    if not is_valid(ckpt_dir, ('.ckpt', '.safetensors')):\n",
        "        cprint(f\"No checkpoints were found in the directory '{ckpt_dir}'.\", color=\"yellow\")\n",
        "        url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/blob/main/animagine-xl-3.1.safetensors\"\n",
        "        filename = get_filename(url)\n",
        "        aria2_download(url=url, download_dir=ckpt_dir, filename=filename)\n",
        "        print_line(80, color=\"green\")\n",
        "        auto_select_model = True\n",
        "\n",
        "    if not is_valid(vae_dir, ('.vae.pt', '.vae.safetensors', '.pt', '.ckpt')):\n",
        "        cprint(f\"No VAEs were found in the directory '{vae_dir}'.\", color=\"yellow\")\n",
        "        url = \"https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl.vae.safetensors\"\n",
        "        filename = get_filename(url)\n",
        "        aria2_download(url=url, download_dir=vae_dir, filename=filename)\n",
        "        print_line(80, color=\"green\")\n",
        "        auto_select_vae = True\n",
        "\n",
        "    if auto_select_model:\n",
        "        selected_model  = auto_select_file(ckpt_dir, \"sd_model_checkpoint\", ('.ckpt', '.safetensors'))\n",
        "        cprint(f\"Selected Model: {selected_model}\", color=\"green\")\n",
        "\n",
        "    if auto_select_vae:\n",
        "        selected_vae    = auto_select_file(vae_dir, \"sd_vae\", ('.vae.pt', '.vae.safetensors', '.pt', '.ckpt'))\n",
        "        cprint(f\"Selected VAE: {selected_vae}\", color=\"green\")\n",
        "\n",
        "    print_line(80, color=\"green\")\n",
        "\n",
        "    general_config_presets(config_file_path, lora_dir, use_presets, ui_config_file_path)\n",
        "\n",
        "    if use_gradio_auth:\n",
        "      cprint(\"Gradio Auth (use this account to login):\", color=\"green\")\n",
        "      cprint(\"[-] Username: cagliostro\", color=\"green\")\n",
        "      cprint(\"[-] Password:\", password, color=\"green\")\n",
        "      print_line(80, color=\"green\")\n",
        "\n",
        "    config = {\n",
        "        \"enable-insecure-extension-access\": True,\n",
        "        \"disable-safe-unpickle\"           : True,\n",
        "        \"share\"                           : True if not ngrok_token else False,\n",
        "        \"ngrok\"                           : ngrok_token if ngrok_token else None,\n",
        "        \"ngrok-region\"                    : ngrok_region if ngrok_token else None,\n",
        "        \"gradio-auth\"                     : f\"{user}:{password}\" if use_gradio_auth else None,\n",
        "        \"no-hashing\"                      : True,\n",
        "        \"disable-console-progressbars\"    : True,\n",
        "        \"lowram\"                          : True,\n",
        "        \"opt-sub-quad-attention\"          : True,\n",
        "        \"opt-channelslast\"                : True,\n",
        "        \"no-download-sd-model\"            : True,\n",
        "        \"gradio-queue\"                    : True,\n",
        "        \"listen\"                          : True,\n",
        "        \"ckpt-dir\"                        : ckpt_dir,\n",
        "        \"vae-dir\"                         : vae_dir,\n",
        "        \"lora-dir\"                        : lora_dir,\n",
        "    }\n",
        "\n",
        "    args = parse_args(config)\n",
        "    final_args = f\"python launch.py {args} {additional_arguments}\"\n",
        "\n",
        "    cprint()\n",
        "    os.chdir(repo_dir)\n",
        "    ! {final_args}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "YYzHDlgEkkrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264ef5e5-21ef-4d18-f04d-67b4d721d5b8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mSet default output path to: /content/drive/MyDrive/cagliostro-colab-forge\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Current GPU: Tesla T4\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Torch 2.6.0+cu124\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mInstalling ubuntu dependencies\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mStable Diffusion Web UI Forge already installed, skipping...\u001b[0m\n",
            "\u001b[0m\u001b[0;32mPreparing environment...\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;31mError while pulling the repository in /content/stable-diffusion-webui-forge: error: Pulling is not possible because you have unmerged files.\n",
            "hint: Fix them up in the work tree, and then use 'git add/rm <file>'\n",
            "hint: as appropriate to mark resolution and make a commit.\n",
            "fatal: Exiting because of an unresolved conflict.\n",
            "\u001b[0m\n",
            "\u001b[0m\u001b[0;32mUsing 'lllyasviel/stable-diffusion-webui-forge' repository...\u001b[0m\n",
            "\u001b[0m\u001b[0;32mBranch: main, Commit hash: d557aef9d889556e5765e5497a6b8187100dbeb5\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0mFinished installation. Took 2 sec.\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Downloading Custom vae...\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mStarting download of 'sdxl.vae.safetensors' with aria2c...\u001b[0m\n",
            "\u001b[0m\u001b[0;32mDownload of 'sdxl.vae.safetensors' completed. Took 0 sec.\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0mDownload finished. Took 0 sec.\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0mLaunching 'lllyasviel/stable-diffusion-webui-forge'\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mSelected VAE: sdxl.vae.safetensors\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\n",
            "Python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Version: f2.0.1v1.10.1-previous-664-gd557aef9\n",
            "Commit hash: d557aef9d889556e5765e5497a6b8187100dbeb5\n",
            "loading WD14-tagger reqs from /content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/requirements.txt\n",
            "Checking WD14-tagger requirements.\n",
            "Legacy Preprocessor init warning: Unable to install insightface automatically. Please try run `pip install insightface` manually.\n",
            "Launching Web UI with arguments: --enable-insecure-extension-access --disable-safe-unpickle --share --no-hashing --disable-console-progressbars --lowram --opt-sub-quad-attention --opt-channelslast --no-download-sd-model --gradio-queue --listen --lowram --theme dark --no-half-vae --opt-sdp-attention\n",
            "Total VRAM 15095 MB, total RAM 52216 MB\n",
            "pytorch version: 2.6.0+cu124\n",
            "Set vram state to: NORMAL_VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype preferences: [torch.float32] -> torch.float32\n",
            "CUDA Using Stream: False\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749347882.632828   14649 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749347882.639060   14649 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Using pytorch cross attention\n",
            "Using pytorch attention for VAE\n",
            "ControlNet preprocessor location: /content/stable-diffusion-webui-forge/models/ControlNetPreprocessor\n",
            "Tag Autocomplete: Could not locate model-keyword extension, Lora trigger word completion will be limited to those added through the extra networks menu.\n",
            "\u001b[38;5;208mâ–¶\u001b[0m SD-Hub: \u001b[38;5;39mv4.9.1\u001b[0m\n",
            "[Vec. CC] Style Sheet Loaded...\n",
            "== WD14 tagger /gpu:0, uname_result(system='Linux', node='87f0fcb865f1', release='6.1.123+', version='#1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025', machine='x86_64') ==\n",
            "2025-06-08 01:58:14,172 - ControlNet - \u001b[0;32mINFO\u001b[0m - ControlNet UI callback registered.\n",
            "SD-Hub : Civitai API Key loaded\n",
            "/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py:232: GradioDeprecationWarning: unexpected argument for HTML: interactive\n",
            "  info = gr.HTML(\n",
            "Checkpoint animagine-xl-3.1.safetensors not found; loading fallback unholyDesireMixSinister_v50.safetensors\n",
            "Model selected: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/unholyDesireMixSinister_v50.safetensors', 'hash': '18635171'}, 'additional_modules': [], 'unet_storage_dtype': None}\n",
            "Using online LoRAs in FP16: False\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://2a6ac19a9407c7ef1d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "\u001b[92mIIB Database file has been successfully backed up to the backup folder.\u001b[0m\n",
            "Startup time: 31.9s (prepare environment: 9.7s, launcher: 0.5s, import torch: 11.0s, other imports: 0.2s, load scripts: 3.2s, create ui: 3.4s, gradio launch: 2.9s, app_started_callback: 0.9s).\n",
            "Environment vars changed: {'stream': False, 'inference_memory': 1024.0, 'pin_shared_memory': False}\n",
            "[GPU Setting] You will use 93.22% GPU memory (14071.00 MB) to load weights, and use 6.78% GPU memory (1024.00 MB) to do matrix computation.\n",
            "Environment vars changed: {'stream': False, 'inference_memory': 1017.0, 'pin_shared_memory': False}\n",
            "[GPU Setting] You will use 93.26% GPU memory (14078.00 MB) to load weights, and use 6.74% GPU memory (1017.00 MB) to do matrix computation.\n",
            "Environment vars changed: {'stream': False, 'inference_memory': 1024.0, 'pin_shared_memory': False}\n",
            "[GPU Setting] You will use 93.22% GPU memory (14071.00 MB) to load weights, and use 6.78% GPU memory (1024.00 MB) to do matrix computation.\n",
            "Loading WD14 moat tagger v2 model file from SmilingWolf/wd-v1-4-moat-tagger-v2, model.onnx\n",
            "model.onnx: 100% 326M/326M [00:00<00:00, 458MB/s]\n",
            "selected_tags.csv: 100% 254k/254k [00:00<00:00, 35.9MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>1.5 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1923, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1508, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 818, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/infotext_utils.py\", line 583, in paste_func\n",
            "    params = parse_generation_parameters(prompt)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/infotext_utils.py\", line 271, in parse_generation_parameters\n",
            "    *lines, lastline = x.strip().split(\"\\n\")\n",
            "                       ^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Loading WD14 moat tagger v2 model file from SmilingWolf/wd-v1-4-moat-tagger-v2, model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>0.6 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Loading WD14 ConvNeXT v1 model file from SmilingWolf/wd-v1-4-convnext-tagger, model.onnx\n",
            "model.onnx: 100% 377M/377M [00:00<00:00, 382MB/s]\n",
            "selected_tags.csv: 100% 174k/174k [00:00<00:00, 56.0MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>1.7 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Loading WD14 ConvNeXT v1 model file from SmilingWolf/wd-v1-4-convnext-tagger, model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>0.5 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Model selected: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/unholyDesireMixSinister_v50.safetensors', 'hash': '18635171'}, 'additional_modules': [], 'unet_storage_dtype': None}\n",
            "Using online LoRAs in FP16: False\n",
            "Loading WD14 ConvNeXT v1 model file from SmilingWolf/wd-v1-4-convnext-tagger, model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>0.5 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "found 0 image(s)\n",
            "found 0 image(s)\n",
            "Tags: 0it [00:00, ?it/s]\n",
            "Loading WD14 ConvNeXT v1 model file from SmilingWolf/wd-v1-4-convnext-tagger, model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>1.1 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Loading WD14 moat tagger v2 model file from SmilingWolf/wd-v1-4-moat-tagger-v2, model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>0.5 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "found 11 image(s)\n",
            "Tags:   0% 0/11 [00:00<?, ?it/s]Loading WD14 moat tagger v2 model file from SmilingWolf/wd-v1-4-moat-tagger-v2, model.onnx\n",
            "Tags:   0% 0/11 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 81, in on_interrogate\n",
            "    interrogator.batch_interrogate()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 223, in batch_interrogate\n",
            "    self.batch_interrogate_image(i)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 196, in batch_interrogate_image\n",
            "    data = (abspath, out_path, fi_key) + self.interrogate(image)\n",
            "                                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 359, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 57, in <module>\n",
            "    raise import_capi_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 23, in <module>\n",
            "    from onnxruntime.capi._pybind_state import ExecutionMode  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/_pybind_state.py\", line 32, in <module>\n",
            "    from .onnxruntime_pybind11_state import *  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "No module named 'onnxruntime.capi.onnxruntime_pybind11_state'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>ModuleNotFoundError: No module named &#x27;onnxruntime.capi.onnxruntime_pybind11_state&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>0.8 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Loading WD14 moat tagger v2 model file from SmilingWolf/wd-v1-4-moat-tagger-v2, model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 340, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 79, in <module>\n",
            "    package_name, version, cuda_version = onnxruntime_validation.get_package_name_and_version_info()\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'onnxruntime.capi.onnxruntime_validation' has no attribute 'get_package_name_and_version_info'\n",
            "module 'onnxruntime.capi.onnxruntime_validation' has no attribute 'get_package_name_and_version_info'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>AttributeError: module &#x27;onnxruntime.capi.onnxruntime_validation&#x27; has no attribute &#x27;get_package_name_and_version_info&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>0.5 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Loading WD14 moat tagger v2 model file from SmilingWolf/wd-v1-4-moat-tagger-v2, model.onnx\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 76, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 55, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/modules/call_queue.py\", line 39, in f\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/ui.py\", line 113, in on_interrogate_image_submit\n",
            "    interrogator.interrogate_image(image)\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 150, in interrogate_image\n",
            "    data = ('', '', fi_key) + self.interrogate(image)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 448, in interrogate\n",
            "    self.load()\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 432, in load\n",
            "    ort = get_onnxrt()\n",
            "          ^^^^^^^^^^^^\n",
            "  File \"/content/stable-diffusion-webui-forge/extensions/stable-diffusion-webui-wd14-tagger/tagger/interrogator.py\", line 340, in get_onnxrt\n",
            "    import onnxruntime\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/onnxruntime/__init__.py\", line 79, in <module>\n",
            "    package_name, version, cuda_version = onnxruntime_validation.get_package_name_and_version_info()\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'onnxruntime.capi.onnxruntime_validation' has no attribute 'get_package_name_and_version_info'\n",
            "module 'onnxruntime.capi.onnxruntime_validation' has no attribute 'get_package_name_and_version_info'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 285, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1933, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1705, in postprocess_data\n",
            "    self.validate_outputs(block_fn, predictions)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1679, in validate_outputs\n",
            "    raise ValueError(\n",
            "ValueError: An event handler (on_interrogate_image_submit) didn't receive enough output values (needed: 7, received: 3).\n",
            "Wanted outputs:\n",
            "    [<gradio.components.state.State object at 0x7df76ddb0fd0>, <gradio.components.html.HTML object at 0x7df76ddb13d0>, <gradio.components.html.HTML object at 0x7df76ddb2710>, <gradio.components.label.Label object at 0x7df76ddb1050>, <gradio.components.label.Label object at 0x7df76e01fad0>, <gradio.components.label.Label object at 0x7df76ddb3790>, <gradio.components.html.HTML object at 0x7df76dd8d2d0>]\n",
            "Received outputs:\n",
            "    [None, \"\", \"<div class='error'>AttributeError: module &#x27;onnxruntime.capi.onnxruntime_validation&#x27; has no attribute &#x27;get_package_name_and_version_info&#x27;</div><div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>0.5 sec.</span></p><p class='vram'><abbr title='Active: peak amount of video memory used during generation (excluding cached data)'>A</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='Reserved: total amount of video memory allocated by the Torch library '>R</abbr>: <span class='measurement'>0.00 GB</span>, <wbr><abbr title='System: peak amount of video memory allocated by all running programs, out of total capacity'>Sys</abbr>: <span class='measurement'>0.1/14.7422 GB</span> (0.8%)</p></div>\"]\n",
            "Loading WD14 moat tagger v2 model file from SmilingWolf/wd-v1-4-moat-tagger-v2, model.onnx\n",
            "Loaded WD14 moat tagger v2 model from SmilingWolf/wd-v1-4-moat-tagger-v2\n",
            "Loading Model: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/unholyDesireMixSinister_v50.safetensors', 'hash': '18635171'}, 'additional_modules': [], 'unet_storage_dtype': None}\n",
            "[Unload] Trying to free all memory for cuda:0 with 0 models keep loaded ... Done.\n",
            "StateDict Keys: {'unet': 1680, 'vae': 248, 'text_encoder': 196, 'text_encoder_2': 518, 'ignore': 0}\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "K-Model Created: {'storage_dtype': torch.float16, 'computation_dtype': torch.float16}\n",
            "Model loaded in 1.4s (unload existing model: 0.6s, forge model load: 0.8s).\n",
            "Downloading VAEApprox model to: /content/stable-diffusion-webui-forge/models/VAE-approx/vaeapprox-sdxl.pt\n",
            "100% 209k/209k [00:00<00:00, 64.6MB/s]\n",
            "[Unload] Trying to free 3051.58 MB for cuda:0 with 0 models keep loaded ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 14972.02 MB, Model Require: 1559.68 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 12388.34 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.49 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 13176.32 MB ... Done.\n",
            "[Unload] Trying to free 7630.80 MB for cuda:0 with 0 models keep loaded ... Current free memory is 13174.82 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 13174.82 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 7253.77 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.64 seconds\n",
            "100% 28/28 [00:23<00:00,  1.18it/s]\n",
            "[Unload] Trying to free 8820.57 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8169.00 MB ... Unload model JointTextEncoder Current free memory is 9929.35 MB ... Done.\n",
            "[Memory Management] Target: IntegratedAutoencoderKL, Free GPU: 9929.35 MB, Model Require: 319.11 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 8586.24 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.43 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KmPAxAtPS6hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pillow\n",
        "!pip uninstall Pillow"
      ],
      "metadata": {
        "id": "9qNPSRTVtfQE",
        "outputId": "1b89c4aa-d027-46bc-8882-f5a3a96517a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Name: pillow\n",
            "Version: 11.2.1\n",
            "Summary: Python Imaging Library (Fork)\n",
            "Home-page: https://python-pillow.github.io\n",
            "Author: \n",
            "Author-email: \"Jeffrey A. Clark\" <aclark@aclark.net>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: \n",
            "Required-by: blendmodes, bokeh, clean-fid, diffusers, dopamine_rl, facexlib, fastai, fvcore, gradio, gradio_imageslider, imageio, matplotlib, reportlab, scikit-image, sentence-transformers, torchtune, torchvision, wordcloud\n",
            "Found existing installation: pillow 11.2.1\n",
            "Uninstalling pillow-11.2.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/AvifImagePlugin.py\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_avif.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imaging.cpython-311-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imaging.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingcms.cpython-311-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingcms.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingft.cpython-311-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingft.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingmath.cpython-311-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingmath.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingmorph.cpython-311-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingmorph.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingtk.cpython-311-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_imagingtk.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_typing.py\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_webp.cpython-311-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/_webp.pyi\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/py.typed\n",
            "    /usr/local/lib/python3.11/dist-packages/PIL/report.py\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow-11.2.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libXau-154567c4.so.6.0.0\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libbrotlicommon-c55a5f7a.so.1.1.0\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libbrotlidec-2ced2f3a.so.1.1.0\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libfreetype-5d47eaee.so.6.20.2\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libharfbuzz-3ac4a098.so.0.61101.0\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libjpeg-b82026ff.so.62.4.0\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/liblcms2-cc10e42f.so.2.0.17\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/liblzma-64b7ab39.so.5.8.1\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libopenjp2-56811f71.so.2.5.3\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libpng16-ade663c1.so.16.47.0\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libsharpyuv-60a7c00b.so.0.1.1\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libtiff-5df1d27b.so.6.1.0\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libwebp-5f0275c0.so.7.1.10\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libwebpdemux-efaed568.so.2.0.16\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libwebpmux-6f2b1ad9.so.3.1.1\n",
            "    /usr/local/lib/python3.11/dist-packages/pillow.libs/libxcb-55eab65a.so.1.1.0\n",
            "Proceed (Y/n)? Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/uninstall.py\", line 106, in run\n",
            "    uninstall_pathset = req.uninstall(\n",
            "                        ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_install.py\", line 722, in uninstall\n",
            "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_uninstall.py\", line 364, in remove\n",
            "    if auto_confirm or self._allowed_to_proceed(verbose):\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_uninstall.py\", line 404, in _allowed_to_proceed\n",
            "    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/misc.py\", line 235, in ask\n",
            "    response = input(message)\n",
            "               ^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## **Download Generated Images**\n",
        "# @markdown Download file manually from files tab or save to Google Drive\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from colablib.colored_print import cprint\n",
        "\n",
        "os.chdir(output_dir)\n",
        "\n",
        "use_drive = False  # @param {type:\"boolean\"}\n",
        "folder_name = \"cagliostro-forge-colab\"  # @param {type: \"string\"}\n",
        "filename = \"waifu.zip\"  # @param {type: \"string\"}\n",
        "save_as = filename\n",
        "\n",
        "def get_unique_filename(base_filename):\n",
        "    path = Path(base_filename)\n",
        "    if not path.exists():\n",
        "        return path\n",
        "    i = 1\n",
        "    while True:\n",
        "        new_path = path.with_name(f\"{path.stem}({i}){path.suffix}\")\n",
        "        if not new_path.exists():\n",
        "            return new_path\n",
        "        i += 1\n",
        "\n",
        "filename = get_unique_filename(filename)\n",
        "\n",
        "def zip_directory(directory, zipname):\n",
        "    with zipfile.ZipFile(zipname, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file_path in directory.rglob('*'):\n",
        "            if file_path.is_file():\n",
        "                zipf.write(file_path, file_path.relative_to(directory.parent))\n",
        "\n",
        "zip_directory(output_dir, Path('/content/outputs.zip'))\n",
        "\n",
        "if use_drive:\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive_service = GoogleDrive(gauth)\n",
        "\n",
        "    def create_folder(folder_name):\n",
        "        query = f\"title='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "        file_list = drive_service.ListFile({\"q\": query}).GetList()\n",
        "        if file_list:\n",
        "            cprint(\"Debug: Folder exists\", color=\"green\")\n",
        "            return file_list[0][\"id\"]\n",
        "        else:\n",
        "            cprint(\"Debug: Creating folder\", color=\"green\")\n",
        "            folder = drive_service.CreateFile({\n",
        "                \"title\": folder_name,\n",
        "                \"mimeType\": \"application/vnd.google-apps.folder\"\n",
        "            })\n",
        "            folder.Upload()\n",
        "            return folder[\"id\"]\n",
        "\n",
        "    def upload_file(file_path, folder_id, save_as):\n",
        "        save_as = get_unique_filename(save_as)\n",
        "        file = drive_service.CreateFile({\"title\": save_as.name, \"parents\": [{\"id\": folder_id}]})\n",
        "        file.SetContentFile(str(file_path))\n",
        "        file.Upload()\n",
        "        file.InsertPermission({\"type\": \"anyone\", \"value\": \"anyone\", \"role\": \"reader\"})\n",
        "        return file[\"id\"]\n",
        "\n",
        "    folder_id = create_folder(folder_name)\n",
        "    file_id = upload_file(Path('/content/outputs.zip'), folder_id, Path(save_as))\n",
        "    sharing_link = f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
        "    cprint(f\"Your sharing link: {sharing_link}\", color=\"green\")\n",
        "else:\n",
        "    cprint(\"Files zipped locally. Download manually from the files tab.\", color=\"yellow\")\n"
      ],
      "metadata": {
        "id": "UyTKsCa1qUL4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}